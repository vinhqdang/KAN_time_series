
\section{Introduction}

In the era of big data and artificial intelligence, organizations are increasingly relying on advanced analytics to drive strategic decision-making. From financial portfolio management to supply chain optimization and macroeconomic forecasting, the ability to accurately predict future states is a critical competitive advantage \cite{Davenport2018}. However, as predictive models have become more sophisticated—evolving from simple linear regressions to complex deep neural networks—a fundamental tension has emerged in the field of Information Systems (IS): the trade-off between predictive accuracy and model interpretability \cite{Rai2020, Shmueli2010}.

Managerial decision-making is rarely purely about prediction; it is fundamentally about understanding causality. A financial manager needs to know not just that a stock price will drop, but *why*—is it driven by interest rate changes, commodity shocks, or market sentiment? A supply chain director needs to understand *which* upstream delays will cascade into downstream shortages. Without this causal understanding, even highly accurate predictions can be dangerous, leading to correct actions for the wrong reasons or an inability to intervene effectively when conditions change \cite{Pearl2009}.

Current artifacts in the IS decision support landscape fall into two distinct categories, leaving a critical gap. On one side are \textbf{statistical and econometric models} (e.g., VAR, ARIMA), which offer transparent causal structures but often lack the capacity to capture complex, non-linear dependencies inherent in modern business data. On the other side are \textbf{deep learning models} (e.g., LSTMs, Transformers), which achieve state-of-the-art predictive accuracy but operate as opaque "black boxes," obscuring the underlying mechanisms driving their outputs \cite{Benbasat2021}. This lack of transparency hinders trust, limits adoption in high-stakes environments, and fails to provide the "explanatory power" required for strategic sense-making \cite{Gregor2006}.

This research addresses this gap by introducing \textbf{Causal Discovery Kolmogorov-Arnold Networks (CD-KAN)}, a novel design science artifact that unifies the interpretability of causal inference with the predictive power of deep learning. Unlike traditional neural networks that rely on fixed activation functions and dense connectivity, CD-KAN leverages the Kolmogorov-Arnold representation theorem to learn distinct, interpretable functions for each causal relationship. Furthermore, we integrate a rigorous, differentiable Directed Acyclic Graph (DAG) learning mechanism that enforces strict causal structure during the training process, rather than attempting to extract it post-hoc.

We position this work within the IS design science paradigm \cite{Hevner2004}, contributing a new class of decision support system that offers "dual capability":
\begin{enumerate}
    \item \textbf{State-of-the-Art Forecasting}: Achieving predictive accuracy superior to leading deep learning baselines (e.g., TSMixer, LSTM) by capturing complex non-linear dynamics.
    \item \textbf{Transparent Causal Discovery}: Automatically recovering interpretable causal graphs that reveal the structural dependencies between organizational variables, enabling managers to trace the "why" behind every prediction.
\end{enumerate}

The contributions of this paper are threefold. First, we provide a \textbf{theoretical contribution} to the literature on Explainable AI (XAI) in IS, proposing that interpretability need not come at the cost of accuracy if the model architecture itself is grounded in causal theory. Second, we offer a \textbf{methodological contribution} through the development of the Augmented Lagrangian Method for time-series DAG learning, a rigorous approach to enforcing structural constraints in neural networks. Third, we demonstrate \textbf{practical utility} through extensive evaluation across ten diverse datasets—including financial markets, cryptocurrency portfolios, and macroeconomic indicators—showing how CD-KAN empowers managers to identify leading indicators and actionable leverage points in complex systems.

The remainder of this paper is organized as follows. Section 2 reviews relevant literature on time series forecasting and causal discovery in IS. Section 3 details the design of the CD-KAN artifact. Section 4 presents the evaluation methodology and comprehensive benchmark results. Section 5 discusses managerial implications and use cases, and Section 6 concludes with limitations and future research directions.
