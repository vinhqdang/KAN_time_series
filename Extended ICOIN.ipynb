{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bb3b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Kafka: 127.0.0.1:9094 | Topic: mln_stream_1758713926_2e3ffe\n",
      "[YF] GLD: 250 rows\n",
      "[YF] CL=F: 250 rows\n",
      "[YF] DX-Y.NYB: 250 rows\n",
      "GLD: train (187, 16), test (47, 16)\n",
      "WTI: train (187, 16), test (47, 16)\n",
      "DXY: train (187, 16), test (47, 16)\n",
      "\n",
      "=== Train on GLD ===\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "[GLD] KAN step 50/150 | loss=0.005610\n",
      "[GLD] KAN step 100/150 | loss=0.003929\n",
      "[GLD] KAN step 150/150 | loss=0.003341\n",
      "[GLD] KAN final train MSE=0.003333 | test MSE=0.004660\n",
      "[GLD] LSTM step 50/150 | loss=0.030687\n",
      "[GLD] LSTM step 100/150 | loss=0.016926\n",
      "[GLD] LSTM step 150/150 | loss=0.011082\n",
      "[GLD] LSTM final train MSE=0.010996 | test MSE=0.013676\n",
      "[GLD] Transformer step 50/150 | loss=0.013016\n",
      "[GLD] Transformer step 100/150 | loss=0.007777\n",
      "[GLD] Transformer step 150/150 | loss=0.004849\n",
      "[GLD] Transformer final train MSE=0.003557 | test MSE=0.005222\n",
      "[GLD] TSMixer step 50/150 | loss=0.005804\n",
      "[GLD] TSMixer step 100/150 | loss=0.003573\n",
      "[GLD] TSMixer step 150/150 | loss=0.003211\n",
      "[GLD] TSMixer final train MSE=0.003207 | test MSE=0.004263\n",
      "\n",
      "=== Train on WTI ===\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "[WTI] KAN step 50/150 | loss=0.006107\n",
      "[WTI] KAN step 100/150 | loss=0.003808\n",
      "[WTI] KAN step 150/150 | loss=0.003010\n",
      "[WTI] KAN final train MSE=0.002998 | test MSE=0.005506\n",
      "[WTI] LSTM step 50/150 | loss=0.035579\n",
      "[WTI] LSTM step 100/150 | loss=0.014259\n",
      "[WTI] LSTM step 150/150 | loss=0.010476\n",
      "[WTI] LSTM final train MSE=0.010426 | test MSE=0.007818\n",
      "[WTI] Transformer step 50/150 | loss=0.010881\n",
      "[WTI] Transformer step 100/150 | loss=0.007082\n",
      "[WTI] Transformer step 150/150 | loss=0.005439\n",
      "[WTI] Transformer final train MSE=0.003495 | test MSE=0.004400\n",
      "[WTI] TSMixer step 50/150 | loss=0.006036\n",
      "[WTI] TSMixer step 100/150 | loss=0.003602\n",
      "[WTI] TSMixer step 150/150 | loss=0.003255\n",
      "[WTI] TSMixer final train MSE=0.003251 | test MSE=0.005015\n",
      "\n",
      "=== Train on DXY ===\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "[DXY] KAN step 50/150 | loss=0.005784\n",
      "[DXY] KAN step 100/150 | loss=0.004024\n",
      "[DXY] KAN step 150/150 | loss=0.003357\n",
      "[DXY] KAN final train MSE=0.003348 | test MSE=0.004696\n",
      "[DXY] LSTM step 50/150 | loss=0.032697\n",
      "[DXY] LSTM step 100/150 | loss=0.012905\n",
      "[DXY] LSTM step 150/150 | loss=0.009429\n",
      "[DXY] LSTM final train MSE=0.009331 | test MSE=0.011110\n",
      "[DXY] Transformer step 50/150 | loss=0.008894\n",
      "[DXY] Transformer step 100/150 | loss=0.005699\n",
      "[DXY] Transformer step 150/150 | loss=0.004755\n",
      "[DXY] Transformer final train MSE=0.003389 | test MSE=0.004682\n",
      "[DXY] TSMixer step 50/150 | loss=0.005939\n",
      "[DXY] TSMixer step 100/150 | loss=0.003823\n",
      "[DXY] TSMixer step 150/150 | loss=0.003406\n",
      "[DXY] TSMixer final train MSE=0.003401 | test MSE=0.004566\n",
      "Kafka reachable: 127.0.0.1:9094\n",
      "Topic ready: mln_stream_1758713926_2e3ffe\n",
      "TARGET_SCORES: {'GLD': 47, 'WTI': 47, 'DXY': 47}\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, asyncio, warnings, uuid, contextlib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as web  # Stooq fallback\n",
    "# Optional FRED fallback if key is set\n",
    "FRED_API_KEY = os.environ.get(\"FRED_API_KEY\", \"\").strip()\n",
    "if FRED_API_KEY:\n",
    "    try:\n",
    "        from fredapi import Fred\n",
    "        _fred = Fred(api_key=FRED_API_KEY)\n",
    "    except Exception:\n",
    "        _fred = None\n",
    "else:\n",
    "    _fred = None\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from aiokafka import AIOKafkaProducer, AIOKafkaConsumer\n",
    "from aiokafka.admin import AIOKafkaAdminClient, NewTopic\n",
    "\n",
    "# ---------- Runtime ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.environ.get(\"KAFKA_BOOTSTRAP\", \"127.0.0.1:9094\")\n",
    "RUN_ID = f\"{int(time.time())}_{uuid.uuid4().hex[:6]}\"\n",
    "TOPIC  = f\"mln_stream_{RUN_ID}\"\n",
    "print(\"Kafka:\", KAFKA_BOOTSTRAP, \"| Topic:\", TOPIC)\n",
    "\n",
    "# Sliding-window + horizon\n",
    "WINDOW  = 16\n",
    "HORIZON = 1  # one-step ahead\n",
    "\n",
    "# Evaluation caps\n",
    "MAX_SCORED = 50     # max scored predictions per asset before stopping\n",
    "\n",
    "\n",
    "# Map for optional FRED daily series if yfinance fails AND you have a FRED key\n",
    "FRED_SERIES = {\n",
    "    \"CL=F\":     \"DCOILWTICO\",   # WTI crude oil spot/daily\n",
    "    \"DX-Y.NYB\": \"DTWEXBGS\",     # Broad Dollar Index daily\n",
    "    \"GLD\":      \"GOLDAMGBD228NLBM\",  # London morning fix (daily)\n",
    "}\n",
    "\n",
    "def _try_yf(ticker, start, end):\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start, end=end, auto_adjust=False, progress=False)\n",
    "        if isinstance(df, pd.DataFrame) and \"Close\" in df.columns and not df[\"Close\"].dropna().empty:\n",
    "            out = df[[\"Close\"]].dropna().copy()\n",
    "            out.index.name = \"Date\"\n",
    "            out.sort_index(inplace=True)\n",
    "            print(f\"[YF] {ticker}: {len(out)} rows\")\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(f\"[YF FAIL] {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "def _try_stooq(ticker, start, end):\n",
    "    try:\n",
    "        df = web.DataReader(ticker, \"stooq\", start=start, end=end)\n",
    "        df = df.sort_index()\n",
    "        cols = {c: c.title() for c in df.columns}\n",
    "        df.rename(columns=cols, inplace=True)\n",
    "        if \"Close\" in df.columns and not df[\"Close\"].dropna().empty:\n",
    "            out = df[[\"Close\"]].dropna().copy()\n",
    "            out.index.name = \"Date\"\n",
    "            print(f\"[STQ] {ticker}: {len(out)} rows\")\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(f\"[STQ FAIL] {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "def _try_fred(ticker, start, end):\n",
    "    if not _fred: \n",
    "        return None\n",
    "    sid = FRED_SERIES.get(ticker)\n",
    "    if not sid:\n",
    "        return None\n",
    "    try:\n",
    "        s = _fred.get_series(sid, observation_start=start, observation_end=end)\n",
    "        s = s.dropna()\n",
    "        if s.empty: \n",
    "            return None\n",
    "        df = pd.DataFrame({\"Close\": s})\n",
    "        # FRED dates may be PeriodIndex or DatetimeIndex; ensure DatetimeIndex and daily freq fill\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.resample(\"D\").ffill().dropna()\n",
    "        df.index.name = \"Date\"\n",
    "        print(f\"[FRED] {ticker}->{sid}: {len(df)} rows\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FRED FAIL] {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_close_series(ticker, start=\"2023-01-01\", end=\"2024-01-01\"):\n",
    "    # 1) yfinance\n",
    "    out = _try_yf(ticker, start, end)\n",
    "    if out is not None: \n",
    "        return out\n",
    "    # 2) stooq fallback (GLD works; CL=F and DX-Y.NYB may not exist on Stooq)\n",
    "    out = _try_stooq(ticker, start, end)\n",
    "    if out is not None:\n",
    "        return out\n",
    "    # 3) FRED (only if key present and mapped)\n",
    "    out = _try_fred(ticker, start, end)\n",
    "    if out is not None:\n",
    "        return out\n",
    "    raise RuntimeError(f\"No real data available for {ticker}. Provide FRED_API_KEY or adjust tickers/date range.\")\n",
    "\n",
    "START, END = \"2023-01-01\", \"2024-01-01\"\n",
    "\n",
    "ASSETS = {\n",
    "    \"GLD\":     {\"ticker\": \"GLD\"},\n",
    "    \"WTI\":     {\"ticker\": \"CL=F\"},\n",
    "    \"DXY\":     {\"ticker\": \"DX-Y.NYB\"},\n",
    "}\n",
    "\n",
    "raw = {}\n",
    "for name, cfg in ASSETS.items():\n",
    "    raw[name] = load_close_series(cfg[\"ticker\"], start=START, end=END)\n",
    "\n",
    "# Per-asset scalers and supervised datasets\n",
    "scalers, datasets = {}, {}\n",
    "\n",
    "def build_supervised(series, window=WINDOW, horizon=HORIZON):\n",
    "    s = np.asarray(series, dtype=np.float32)\n",
    "    X, y = [], []\n",
    "    for i in range(len(s) - window - horizon + 1):\n",
    "        X.append(s[i:i+window])\n",
    "        y.append(s[i+window:i+window+horizon])\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32).reshape(-1, horizon)\n",
    "\n",
    "for asset, df in raw.items():\n",
    "    sc = MinMaxScaler()\n",
    "    scaled = sc.fit_transform(df[[\"Close\"]]).astype(np.float32).reshape(-1)\n",
    "    scalers[asset] = sc\n",
    "    X, y = build_supervised(scaled, WINDOW, HORIZON)\n",
    "    split = int(len(X)*0.8)\n",
    "    datasets[asset] = {\n",
    "        \"df\": df,\n",
    "        \"scaled\": scaled,\n",
    "        \"X_train\": torch.tensor(X[:split], dtype=torch.float32, device=device),\n",
    "        \"y_train\": torch.tensor(y[:split], dtype=torch.float32, device=device),\n",
    "        \"X_test\":  torch.tensor(X[split:], dtype=torch.float32, device=device),\n",
    "        \"y_test\":  torch.tensor(y[split:], dtype=torch.float32, device=device),\n",
    "        \"split\":   split,\n",
    "        \"X_all\":   X, \"y_all\": y   # numpy for stream slicing\n",
    "    }\n",
    "    print(f\"{asset}: train {X[:split].shape}, test {X[split:].shape}\")\n",
    "\n",
    "\n",
    "# --- KAN wrapper (your package must be installed/importable) ---\n",
    "from kan import KAN\n",
    "\n",
    "class KANForecaster(nn.Module):\n",
    "    def __init__(self, window=WINDOW, hidden=32, horizon=HORIZON, seed=42):\n",
    "        super().__init__()\n",
    "        self.kan = KAN(width=[window, hidden, horizon], grid=2, k=3, seed=seed, device=device.type)\n",
    "\n",
    "    def forward(self, x):                  # x: [B, W]\n",
    "        return self.kan(x)\n",
    "\n",
    "# class KANForecaster(nn.Module):\n",
    "#     def __init__(self, window, hidden=16, horizon=1, grid=1, k=2, seed=42, device_str=\"cpu\"):\n",
    "#         super().__init__()\n",
    "#         # Keep width small + grid=1 + k=2 for speed\n",
    "#         self.kan = KAN(width=[window, hidden, horizon],\n",
    "#                        grid=grid, k=k, seed=seed, device=device_str)\n",
    "#     def forward(self, x):\n",
    "#         return self.kan(x)\n",
    "\n",
    "# --- LSTM baseline ---\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden=64, num_layers=1, horizon=HORIZON):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden, num_layers, batch_first=True)\n",
    "        self.fc   = nn.Linear(hidden, horizon)\n",
    "    def forward(self, x):                  # x: [B, W]\n",
    "        x = x.unsqueeze(-1)               # [B, W, 1]\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]              # [B, H]\n",
    "        return self.fc(last)              # [B, horizon]\n",
    "\n",
    "# --- Simple Transformer forecaster (Encoder-only) ---\n",
    "class TransformerForecaster(nn.Module):\n",
    "    def __init__(self, window=WINDOW, d_model=32, nhead=4, num_layers=2, dim_ff=64, horizon=HORIZON):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(1, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, window, d_model))\n",
    "        self.fc = nn.Linear(d_model, horizon)\n",
    "    def forward(self, x):                  # x: [B, W]\n",
    "        x = x.unsqueeze(-1)               # [B, W, 1]\n",
    "        z = self.embed(x) + self.pos      # [B, W, d_model]\n",
    "        z = self.encoder(z)               # [B, W, d_model]\n",
    "        last = z[:, -1, :]                # [B, d_model]\n",
    "        return self.fc(last)              # [B, horizon]\n",
    "\n",
    "# --- Simple TS-Mixer forecaster ---\n",
    "class TSMixer(nn.Module):\n",
    "    def __init__(self, window=WINDOW, d_feat=16, time_hidden=64, chan_hidden=32, num_blocks=2, horizon=HORIZON):\n",
    "        super().__init__()\n",
    "        self.proj_in  = nn.Linear(1, d_feat)\n",
    "        self.time_mlps = nn.ModuleList([nn.Sequential(\n",
    "            nn.LayerNorm([window, d_feat]),\n",
    "            nn.Linear(window, time_hidden), nn.GELU(),\n",
    "            nn.Linear(time_hidden, window)\n",
    "        ) for _ in range(num_blocks)])\n",
    "        self.chan_mlps = nn.ModuleList([nn.Sequential(\n",
    "            nn.LayerNorm([window, d_feat]),\n",
    "            nn.Linear(d_feat, chan_hidden), nn.GELU(),\n",
    "            nn.Linear(chan_hidden, d_feat)\n",
    "        ) for _ in range(num_blocks)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(window*d_feat, 64), nn.GELU(), nn.Linear(64, horizon)\n",
    "        )\n",
    "    def forward(self, x):                  # x: [B, W]\n",
    "        x = x.unsqueeze(-1)               # [B, W, 1]\n",
    "        z = self.proj_in(x)               # [B, W, F]\n",
    "        for tmlp, cmlp in zip(self.time_mlps, self.chan_mlps):\n",
    "            z = z + tmlp(z.transpose(1,2)).transpose(1,2)  # mix over time\n",
    "            z = z + cmlp(z)                                # mix over channels\n",
    "        return self.head(z)               # [B, horizon]\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "\n",
    "def train_once(model, Xtr, ytr, Xte, yte, steps, lr, name):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.train()\n",
    "    for i in range(steps):\n",
    "        opt.zero_grad()\n",
    "        out = model(Xtr)\n",
    "        loss = loss_fn(out, ytr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"{name} step {i+1}/{steps} | loss={loss.item():.6f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tr = loss_fn(model(Xtr), ytr).item()\n",
    "        te = loss_fn(model(Xte), yte).item()\n",
    "    print(f\"{name} final train MSE={tr:.6f} | test MSE={te:.6f}\")\n",
    "\n",
    "MODELS = [\"KAN\",\"LSTM\",\"Transformer\",\"TSMixer\"]\n",
    "models = {m: {} for m in MODELS}\n",
    "\n",
    "for asset, d in datasets.items():\n",
    "    Xtr, ytr, Xte, yte = d[\"X_train\"], d[\"y_train\"], d[\"X_test\"], d[\"y_test\"]\n",
    "    print(f\"\\n=== Train on {asset} ===\")\n",
    "    m_kan  = KANForecaster(WINDOW).to(device)\n",
    "    m_lstm = LSTMForecaster().to(device)\n",
    "    m_trf  = TransformerForecaster(WINDOW).to(device)\n",
    "    m_mix  = TSMixer(WINDOW).to(device)\n",
    "\n",
    "    train_once(m_kan,  Xtr, ytr, Xte, yte, steps=150, lr=1e-2, name=f\"[{asset}] KAN\")\n",
    "    train_once(m_lstm, Xtr, ytr, Xte, yte, steps=150, lr=1e-3, name=f\"[{asset}] LSTM\")\n",
    "    train_once(m_trf,  Xtr, ytr, Xte, yte, steps=150, lr=1e-3, name=f\"[{asset}] Transformer\")\n",
    "    train_once(m_mix,  Xtr, ytr, Xte, yte, steps=150, lr=2e-3, name=f\"[{asset}] TSMixer\")\n",
    "\n",
    "    models[\"KAN\"][asset]          = m_kan.eval()\n",
    "    models[\"LSTM\"][asset]         = m_lstm.eval()\n",
    "    models[\"Transformer\"][asset]  = m_trf.eval()\n",
    "    models[\"TSMixer\"][asset]      = m_mix.eval()\n",
    "\n",
    "\n",
    "async def ensure_topic(name, bootstrap=KAFKA_BOOTSTRAP, partitions=1, rf=1):\n",
    "    admin = AIOKafkaAdminClient(bootstrap_servers=bootstrap)\n",
    "    await admin.start()\n",
    "    try:\n",
    "        topics = await admin.list_topics()\n",
    "        if name not in topics:\n",
    "            await admin.create_topics([NewTopic(name=name, num_partitions=partitions, replication_factor=rf)])\n",
    "            for _ in range(50):\n",
    "                if name in await admin.list_topics():\n",
    "                    break\n",
    "                await asyncio.sleep(0.2)\n",
    "    finally:\n",
    "        await admin.close()\n",
    "\n",
    "async def kafka_smoke(bootstrap=KAFKA_BOOTSTRAP):\n",
    "    prod = AIOKafkaProducer(bootstrap_servers=bootstrap, value_serializer=lambda v: json.dumps(v).encode())\n",
    "    await prod.start()\n",
    "    await prod.stop()\n",
    "    print(\"Kafka reachable:\", bootstrap)\n",
    "\n",
    "await ensure_topic(TOPIC, bootstrap=KAFKA_BOOTSTRAP, partitions=1, rf=1)\n",
    "await kafka_smoke()\n",
    "print(\"Topic ready:\", TOPIC)\n",
    "\n",
    "##### cell 8\n",
    "# ============================\n",
    "# Cell 8 — Producer & Consumer (multi-asset, with per-asset limits)\n",
    "# ============================\n",
    "\n",
    "# ---- Per-asset target scores = min(MAX_SCORED, len(test set)) ----\n",
    "TARGET_SCORES = {\n",
    "    asset: int(min(MAX_SCORED, len(datasets[asset][\"X_test\"])))\n",
    "    for asset in ASSETS\n",
    "}\n",
    "print(\"TARGET_SCORES:\", TARGET_SCORES)\n",
    "\n",
    "# Buckets (cleared in Cell 9)\n",
    "PRED     = {m: {a: [] for a in ASSETS} for m in MODELS}\n",
    "TRUTH    = {a: [] for a in ASSETS}\n",
    "INFER_MS = {m: {a: [] for a in ASSETS} for m in MODELS}\n",
    "E2E_MS   = {a: [] for a in ASSETS}\n",
    "\n",
    "def stream_slice(asset):\n",
    "    \"\"\"Return exactly WINDOW + TARGET_SCORES[asset] scaled points.\"\"\"\n",
    "    d   = datasets[asset]\n",
    "    sc  = d[\"scaled\"]\n",
    "    split = d[\"split\"]\n",
    "    need = WINDOW + TARGET_SCORES[asset]\n",
    "    start_idx = max(0, split - (WINDOW - 1))\n",
    "    end_idx   = start_idx + need\n",
    "    return sc[start_idx:end_idx]\n",
    "\n",
    "def round_robin_payloads():\n",
    "    \"\"\"Interleave per-asset sequences up to their target lengths.\"\"\"\n",
    "    seqs = {a: stream_slice(a) for a in ASSETS}\n",
    "    idx  = {a: 0 for a in ASSETS}\n",
    "    produced = 0\n",
    "    hard_cap = sum(len(v) for v in seqs.values())\n",
    "    while produced < hard_cap:\n",
    "        for a in ASSETS:\n",
    "            if idx[a] < len(seqs[a]):\n",
    "                yield a, float(seqs[a][idx[a]])\n",
    "                idx[a] += 1\n",
    "                produced += 1\n",
    "\n",
    "async def producer(bootstrap=KAFKA_BOOTSTRAP, topic=TOPIC, sleep_s=0.0):\n",
    "    prod = AIOKafkaProducer(\n",
    "        bootstrap_servers=bootstrap,\n",
    "        value_serializer=lambda v: json.dumps(v).encode(),\n",
    "        acks=0, linger_ms=0, compression_type=None\n",
    "    )\n",
    "    await prod.start()\n",
    "    try:\n",
    "        sent = 0\n",
    "        for asset, val in round_robin_payloads():\n",
    "            msg = {\"ts\": time.time(), \"asset\": asset, \"val\": val}\n",
    "            await prod.send_and_wait(topic, msg)\n",
    "            sent += 1\n",
    "            if sleep_s > 0:\n",
    "                await asyncio.sleep(sleep_s)\n",
    "        await asyncio.sleep(0.25)  # drain\n",
    "        print(\"Producer sent:\", sent)\n",
    "    finally:\n",
    "        await prod.stop()\n",
    "\n",
    "async def consumer(bootstrap=KAFKA_BOOTSTRAP, topic=TOPIC):\n",
    "    cons = AIOKafkaConsumer(\n",
    "        topic,\n",
    "        bootstrap_servers=bootstrap,\n",
    "        value_deserializer=lambda v: json.loads(v.decode()),\n",
    "        auto_offset_reset=\"latest\",\n",
    "        group_id=None,\n",
    "        enable_auto_commit=False,\n",
    "        client_id=\"mln_eval_consumer\",\n",
    "        fetch_max_wait_ms=1,\n",
    "        fetch_min_bytes=1,\n",
    "        request_timeout_ms=15000\n",
    "    )\n",
    "    await cons.start()\n",
    "    # seek to end to ignore stale data\n",
    "    while not cons.assignment():\n",
    "        await asyncio.sleep(0.01)\n",
    "    end_offsets = await cons.end_offsets(cons.assignment())\n",
    "    for tp, off in end_offsets.items():\n",
    "        cons.seek(tp, off)\n",
    "    await asyncio.sleep(0.5)  # allow fetcher to settle\n",
    "\n",
    "    # Per-asset buffers, pending predictions, and scored counters\n",
    "    buf     = {a: [] for a in ASSETS}\n",
    "    pending = {a: None for a in ASSETS}\n",
    "    scored  = {a: 0 for a in ASSETS}\n",
    "\n",
    "    try:\n",
    "        async for msg in cons:\n",
    "            recv_t = time.time()\n",
    "            datum  = msg.value\n",
    "            if not (isinstance(datum, dict) and \"asset\" in datum and \"val\" in datum):\n",
    "                continue\n",
    "            asset = datum[\"asset\"]\n",
    "            val   = float(datum[\"val\"])\n",
    "            sent_ts = float(datum.get(\"ts\", recv_t))\n",
    "\n",
    "            # If this asset already reached its target, ignore further ticks for it\n",
    "            if scored[asset] >= TARGET_SCORES[asset]:\n",
    "                continue\n",
    "\n",
    "            # finalize previous prediction for this asset with current truth\n",
    "            if pending[asset] is not None:\n",
    "                preds_per_model, pred_sent_ts = pending[asset]\n",
    "                true_val = scalers[asset].inverse_transform([[val]])[0,0]\n",
    "                TRUTH[asset].append(true_val)\n",
    "\n",
    "                for mname, yhat_scaled, t0, t1 in preds_per_model:\n",
    "                    yhat = scalers[asset].inverse_transform([[yhat_scaled]])[0,0]\n",
    "                    PRED[mname][asset].append(yhat)\n",
    "                    INFER_MS[mname][asset].append((t1 - t0)*1000.0)\n",
    "\n",
    "                E2E_MS[asset].append((recv_t - pred_sent_ts)*1000.0)\n",
    "                scored[asset] += 1\n",
    "                pending[asset] = None\n",
    "\n",
    "                # If ALL assets met their targets, we can stop\n",
    "                if all(scored[a] >= TARGET_SCORES[a] for a in ASSETS):\n",
    "                    break\n",
    "\n",
    "            # issue next prediction if window is ready and we still need predictions\n",
    "            buf[asset].append(val)\n",
    "            if len(buf[asset]) >= WINDOW and scored[asset] < TARGET_SCORES[asset]:\n",
    "                xin = torch.tensor(buf[asset][-WINDOW:], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                preds = []\n",
    "                # KAN\n",
    "                t0 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    yk = models[\"KAN\"][asset](xin).detach().cpu().numpy().flatten()[0]\n",
    "                t1 = time.time()\n",
    "                preds.append((\"KAN\", yk, t0, t1))\n",
    "                # LSTM\n",
    "                t0 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    yl = models[\"LSTM\"][asset](xin).detach().cpu().numpy().flatten()[0]\n",
    "                t1 = time.time()\n",
    "                preds.append((\"LSTM\", yl, t0, t1))\n",
    "                # Transformer\n",
    "                t0 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    yt = models[\"Transformer\"][asset](xin).detach().cpu().numpy().flatten()[0]\n",
    "                t1 = time.time()\n",
    "                preds.append((\"Transformer\", yt, t0, t1))\n",
    "                # TSMixer\n",
    "                t0 = time.time()\n",
    "                with torch.no_grad():\n",
    "                    ym = models[\"TSMixer\"][asset](xin).detach().cpu().numpy().flatten()[0]\n",
    "                t1 = time.time()\n",
    "                preds.append((\"TSMixer\", ym, t0, t1))\n",
    "\n",
    "                pending[asset] = (preds, sent_ts)\n",
    "\n",
    "    finally:\n",
    "        await cons.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer sent: 189\n",
      "\n",
      "=== GLD ===\n",
      "scored truths: 47\n",
      "      Model  MAPE (%)     RMSE      MAE  Avg Inference (ms)  p50 Inference (ms)  p95 Inference (ms)  Samples\n",
      "        KAN  0.616697 1.493197 1.126402           22.699331           22.143841           26.733541       47\n",
      "    TSMixer  0.623301 1.501040 1.140517            0.238231            0.230789            0.336051       47\n",
      "Transformer  0.660299 1.602007 1.206494            0.326101            0.321865            0.390434       47\n",
      "       LSTM  1.382118 3.058881 2.508486            0.384818            0.371933            0.543594       47\n",
      "E2E Latency  avg=841.58 ms | p50=863.36 ms | p95=1644.67 ms\n",
      "\n",
      "=== WTI ===\n",
      "scored truths: 47\n",
      "      Model  MAPE (%)     RMSE      MAE  Avg Inference (ms)  p50 Inference (ms)  p95 Inference (ms)  Samples\n",
      "        KAN  2.081370 2.082632 1.658810           22.058152           21.957159           23.695064       47\n",
      "    TSMixer  2.115231 2.085316 1.694512            0.230845            0.226974            0.281692       47\n",
      "Transformer  2.171696 2.141440 1.742636            0.331615            0.325918            0.448990       47\n",
      "       LSTM  2.972903 2.943886 2.371456            0.372009            0.372887            0.485706       47\n",
      "E2E Latency  avg=846.20 ms | p50=829.46 ms | p95=1667.54 ms\n",
      "\n",
      "=== DXY ===\n",
      "scored truths: 47\n",
      "      Model  MAPE (%)     RMSE      MAE  Avg Inference (ms)  p50 Inference (ms)  p95 Inference (ms)  Samples\n",
      "Transformer  0.324467 0.456793 0.340644            0.325705            0.313759            0.424385       47\n",
      "    TSMixer  0.336428 0.451100 0.353838            0.233173            0.221968            0.284600       47\n",
      "        KAN  0.343458 0.456002 0.361075           22.621784           22.167921           25.780463       47\n",
      "       LSTM  0.487241 0.631629 0.510899            0.368950            0.362873            0.470090       47\n",
      "E2E Latency  avg=839.70 ms | p50=851.77 ms | p95=1621.27 ms\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Orchestrate & Summarize (consumer FIRST)\n",
    "# ============================\n",
    "\n",
    "def summarize():\n",
    "    def _mape(y, yhat):\n",
    "        y, yhat = np.asarray(y), np.asarray(yhat)\n",
    "        if len(y) == 0: return np.nan\n",
    "        return float(np.mean(np.abs((y - yhat) / (np.abs(y) + 1e-8))) * 100.0)\n",
    "\n",
    "    for asset in ASSETS:\n",
    "        y_true = TRUTH[asset]\n",
    "        print(f\"\\n=== {asset} ===\")\n",
    "        print(f\"scored truths: {len(y_true)}\")\n",
    "        rows = []\n",
    "        for m in MODELS:\n",
    "            yhat = PRED[m][asset]\n",
    "            mae  = mean_absolute_error(y_true, yhat) if len(yhat) else np.nan\n",
    "            rmse = mean_squared_error(y_true, yhat, squared=False) if len(yhat) else np.nan\n",
    "            mp   = _mape(y_true, yhat)\n",
    "            infs = INFER_MS[m][asset]\n",
    "            rows.append({\n",
    "                \"Model\": m,\n",
    "                \"MAPE (%)\": mp,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"Avg Inference (ms)\": float(np.mean(infs)) if infs else np.nan,\n",
    "                \"p50 Inference (ms)\": float(np.quantile(infs, 0.50)) if infs else np.nan,\n",
    "                \"p95 Inference (ms)\": float(np.quantile(infs, 0.95)) if infs else np.nan,\n",
    "                \"Samples\": len(yhat)\n",
    "            })\n",
    "        df = pd.DataFrame(rows).sort_values(\"MAPE (%)\")\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "        lat = E2E_MS[asset]\n",
    "        if len(lat):\n",
    "            print(f\"E2E Latency  avg={np.mean(lat):.2f} ms | p50={np.quantile(lat,0.5):.2f} ms | p95={np.quantile(lat,0.95):.2f} ms\")\n",
    "        else:\n",
    "            print(\"E2E Latency  (no samples)\")\n",
    "\n",
    "async def main(timeout_s=90.0, sleep_between_msgs=0.0):\n",
    "    # Ensure the topic exists (idempotent)\n",
    "    await ensure_topic(TOPIC, bootstrap=KAFKA_BOOTSTRAP, partitions=1, rf=1)\n",
    "\n",
    "    # Clear buckets in case of reruns\n",
    "    for m in MODELS:\n",
    "        for a in ASSETS:\n",
    "            PRED[m][a].clear()\n",
    "            INFER_MS[m][a].clear()\n",
    "    for a in ASSETS:\n",
    "        TRUTH[a].clear()\n",
    "        E2E_MS[a].clear()\n",
    "\n",
    "    # 1) Start CONSUMER first so it seeks to end and waits for new data\n",
    "    cons_task = asyncio.create_task(consumer())\n",
    "    await asyncio.sleep(0.7)  # small barrier to settle\n",
    "\n",
    "    # 2) Start PRODUCER (bounded stream by TARGET_SCORES)\n",
    "    prod_task = asyncio.create_task(producer(sleep_s=sleep_between_msgs))\n",
    "\n",
    "    # 3) Wait bounded time (to avoid forever hanging)\n",
    "    try:\n",
    "        await asyncio.wait_for(asyncio.gather(prod_task, cons_task), timeout=timeout_s)\n",
    "    except (asyncio.TimeoutError, asyncio.CancelledError):\n",
    "        print(f\"[WARN] Timed out after {timeout_s:.1f}s or cancelled; using partial results.\")\n",
    "        with contextlib.suppress(Exception):\n",
    "            await prod_task\n",
    "        with contextlib.suppress(Exception):\n",
    "            await cons_task\n",
    "\n",
    "    summarize()\n",
    "\n",
    "# Run\n",
    "await main(timeout_s=120.0, sleep_between_msgs=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paired t-tests: KAN vs baselines on GLD ===\n",
      "   Baseline  n  KAN mean APE (%)  LSTM mean APE (%)  ΔAPE mean (KAN - Baseline) (%)      t      p Holm α=0.05 (KAN better?)  Cohen d (paired)  Transformer mean APE (%)  TSMixer mean APE (%)\n",
      "       LSTM 47             0.617              1.382                          -0.765 -5.261 0.0000                       YES            -0.767                       NaN                   NaN\n",
      "Transformer 47             0.617                NaN                          -0.044 -1.021 0.1563                        no            -0.149                      0.66                   NaN\n",
      "    TSMixer 47             0.617                NaN                          -0.007 -0.257 0.3992                        no            -0.037                       NaN                 0.623\n",
      "→ Against LSTM, KAN is significantly better on GLD (p=1.82e-06).\n",
      "→ Against Transformer, KAN is not significantly better on GLD (p=0.1563).\n",
      "→ Against TSMixer, KAN is not significantly better on GLD (p=0.3992).\n",
      "\n",
      "=== Paired t-tests: KAN vs baselines on WTI ===\n",
      "   Baseline  n  KAN mean APE (%)  LSTM mean APE (%)  ΔAPE mean (KAN - Baseline) (%)      t      p Holm α=0.05 (KAN better?)  Cohen d (paired)  Transformer mean APE (%)  TSMixer mean APE (%)\n",
      "       LSTM 47             2.081              2.973                          -0.892 -2.438 0.0094                       YES            -0.356                       NaN                   NaN\n",
      "Transformer 47             2.081                NaN                          -0.090 -0.589 0.2795                        no            -0.086                     2.172                   NaN\n",
      "    TSMixer 47             2.081                NaN                          -0.034 -0.266 0.3958                        no            -0.039                       NaN                 2.115\n",
      "→ Against LSTM, KAN is significantly better on WTI (p=0.009355).\n",
      "→ Against Transformer, KAN is not significantly better on WTI (p=0.2795).\n",
      "→ Against TSMixer, KAN is not significantly better on WTI (p=0.3958).\n",
      "\n",
      "=== Paired t-tests: KAN vs baselines on DXY ===\n",
      "   Baseline  n  KAN mean APE (%)  LSTM mean APE (%)  ΔAPE mean (KAN - Baseline) (%)      t      p Holm α=0.05 (KAN better?)  Cohen d (paired)  Transformer mean APE (%)  TSMixer mean APE (%)\n",
      "       LSTM 47             0.343              0.487                          -0.144 -3.079 0.0017                       YES            -0.449                       NaN                   NaN\n",
      "    TSMixer 47             0.343                NaN                           0.007  0.653 0.7414                        no             0.095                       NaN                 0.336\n",
      "Transformer 47             0.343                NaN                           0.019  0.809 0.7887                        no             0.118                     0.324                   NaN\n",
      "→ Against LSTM, KAN is significantly better on DXY (p=0.001749).\n",
      "→ Against TSMixer, KAN is not significantly better on DXY (p=0.7414).\n",
      "→ Against Transformer, KAN is not significantly better on DXY (p=0.7887).\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Statistical tests (KAN vs baselines), per asset\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SciPy may not be preinstalled in some base envs\n",
    "try:\n",
    "    from scipy.stats import ttest_rel\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy\"], check=True)\n",
    "    from scipy.stats import ttest_rel\n",
    "\n",
    "def ape_series(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    n = min(len(y_true), len(y_pred))\n",
    "    if n == 0:\n",
    "        return np.array([], dtype=np.float64)\n",
    "    y_true = y_true[:n]\n",
    "    y_pred = y_pred[:n]\n",
    "    return np.abs((y_true - y_pred) / (np.abs(y_true) + eps))\n",
    "\n",
    "def cohen_d_paired(a, b):\n",
    "    # effect size for paired samples (mean of diffs / std of diffs)\n",
    "    d = np.asarray(a) - np.asarray(b)\n",
    "    if len(d) < 2 or np.std(d, ddof=1) == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(d) / np.std(d, ddof=1))\n",
    "\n",
    "def holm_bonferroni(pvals, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Holm step-down: returns list of booleans (reject or not) in the *original* order.\n",
    "    \"\"\"\n",
    "    idx_sorted = np.argsort(pvals)\n",
    "    rejs = [False]*len(pvals)\n",
    "    for k, i in enumerate(idx_sorted, start=1):\n",
    "        if pvals[i] <= alpha / (len(pvals) - k + 1):\n",
    "            rejs[i] = True\n",
    "        else:\n",
    "            # once we hit a non-reject, all larger p's are non-reject\n",
    "            break\n",
    "    return rejs\n",
    "\n",
    "def ttest_kan_vs_baselines_for_asset(asset, baselines=(\"LSTM\", \"Transformer\", \"TSMixer\")):\n",
    "    results = []\n",
    "    y_true = TRUTH[asset]\n",
    "    ape_kan = ape_series(y_true, PRED[\"KAN\"][asset])\n",
    "\n",
    "    if len(ape_kan) == 0:\n",
    "        return pd.DataFrame(columns=[\"Baseline\",\"n\",\"KAN mean APE\",\"Baseline mean APE\",\"ΔAPE (mean)\",\"t\",\"p\",\"Holm α=0.05\",\"Cohen d\"])\n",
    "\n",
    "    pvals = []\n",
    "    rows_tmp = []\n",
    "\n",
    "    for b in baselines:\n",
    "        ape_b = ape_series(y_true, PRED[b][asset])\n",
    "        n = min(len(ape_kan), len(ape_b))\n",
    "        if n < 3:\n",
    "            rows_tmp.append((b, n, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "            pvals.append(1.0)\n",
    "            continue\n",
    "\n",
    "        # paired t-test on sample-wise APE\n",
    "        t_stat, p_val = ttest_rel(ape_kan[:n], ape_b[:n], alternative='less')  \n",
    "        # 'less' because H1: KAN APE < Baseline APE\n",
    "\n",
    "        delta = float(np.mean(ape_kan[:n] - ape_b[:n]))  # negative means KAN better\n",
    "        d_eff = cohen_d_paired(ape_kan[:n], ape_b[:n])\n",
    "\n",
    "        rows_tmp.append((b, n,\n",
    "                         float(np.mean(ape_kan[:n]))*100.0,\n",
    "                         float(np.mean(ape_b[:n]))*100.0,\n",
    "                         delta*100.0,   # report in %\n",
    "                         float(t_stat),\n",
    "                         float(p_val),\n",
    "                         None,          # placeholder for Holm decision\n",
    "                         d_eff))\n",
    "        pvals.append(p_val)\n",
    "\n",
    "    # Holm–Bonferroni across the baselines for this asset\n",
    "    decisions = holm_bonferroni(pvals, alpha=0.05)\n",
    "    out_rows = []\n",
    "    for (b, n, mkan, mbase, dlt, t_stat, p_val, _, d_eff), rej in zip(rows_tmp, decisions):\n",
    "        out_rows.append({\n",
    "            \"Baseline\": b,\n",
    "            \"n\": n,\n",
    "            \"KAN mean APE (%)\": mkan,\n",
    "            f\"{b} mean APE (%)\": mbase,\n",
    "            \"ΔAPE mean (KAN - Baseline) (%)\": dlt,\n",
    "            \"t\": t_stat,\n",
    "            \"p\": p_val,\n",
    "            \"Holm α=0.05 (KAN better?)\": \"YES\" if rej else \"no\",\n",
    "            \"Cohen d (paired)\": d_eff\n",
    "        })\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "# ---- Run tests per asset and print nicely\n",
    "for asset in ASSETS:\n",
    "    print(f\"\\n=== Paired t-tests: KAN vs baselines on {asset} ===\")\n",
    "    df_tests = ttest_kan_vs_baselines_for_asset(asset)\n",
    "    if df_tests.empty:\n",
    "        print(\"Not enough samples to test.\")\n",
    "        continue\n",
    "    # Order by p-value for quick reading\n",
    "    df_tests = df_tests.sort_values(\"p\")\n",
    "    # Round a bit for readability\n",
    "    with pd.option_context('display.max_columns', None):\n",
    "        print(df_tests.round({\n",
    "            \"KAN mean APE (%)\": 3,\n",
    "            f\"LSTM mean APE (%)\": 3 if \"LSTM\" in df_tests.columns else 3,\n",
    "            f\"Transformer mean APE (%)\": 3 if \"Transformer\" in df_tests.columns else 3,\n",
    "            f\"TSMixer mean APE (%)\": 3 if \"TSMixer\" in df_tests.columns else 3,\n",
    "            \"ΔAPE mean (KAN - Baseline) (%)\": 3,\n",
    "            \"t\": 3, \"p\": 4, \"Cohen d (paired)\": 3\n",
    "        }).to_string(index=False))\n",
    "\n",
    "    # Quick interpretation line\n",
    "    for _, r in df_tests.iterrows():\n",
    "        verdict = \"significantly better\" if r[\"Holm α=0.05 (KAN better?)\"] == \"YES\" else \"not significantly better\"\n",
    "        print(f\"→ Against {r['Baseline']}, KAN is {verdict} on {asset} (p={r['p']:.4g}).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
